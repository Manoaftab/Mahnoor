{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smplx in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.28)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: open3d in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smplx) (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: dash>=2.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from open3d) (2.18.2)\n",
      "Requirement already satisfied: werkzeug>=2.2.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from open3d) (3.0.6)\n",
      "Requirement already satisfied: nbformat>=5.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from open3d) (5.10.4)\n",
      "Requirement already satisfied: configargparse in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from open3d) (1.7)\n",
      "Requirement already satisfied: ipywidgets>=8.0.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from open3d) (8.1.5)\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dash>=2.6.0->open3d) (3.0.3)\n",
      "Requirement already satisfied: plotly>=5.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
      "Requirement already satisfied: dash-html-components==2.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
      "Requirement already satisfied: dash-core-components==2.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
      "Requirement already satisfied: dash-table==5.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dash>=2.6.0->open3d) (5.0.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dash>=2.6.0->open3d) (8.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
      "Requirement already satisfied: retrying in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dash>=2.6.0->open3d) (1.3.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dash>=2.6.0->open3d) (65.5.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets>=8.0.4->open3d) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets>=8.0.4->open3d) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets>=8.0.4->open3d) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets>=8.0.4->open3d) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat>=5.7.0->open3d) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=2.2.3->open3d) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (1.9.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.4.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.20.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (306)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (24.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->dash>=2.6.0->open3d) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->dash>=2.6.0->open3d) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->dash>=2.6.0->open3d) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from retrying->dash>=2.6.0->open3d) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install smplx torch open3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set the device (ensure you are using the same device as during training)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "class TextToMotionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, num_layers=4, num_heads=8):\n",
    "        super(TextToMotionTransformer, self).__init__()\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Linear layers for predicting separate motion parameters\n",
    "        self.fc_root = nn.Linear(input_dim, 3)          # Root pose (3)\n",
    "        self.fc_body = nn.Linear(input_dim, 63)         # Body pose (63)\n",
    "        self.fc_left_hand = nn.Linear(input_dim, 45)    # Left hand pose (45)\n",
    "        self.fc_right_hand = nn.Linear(input_dim, 45)   # Right hand pose (45)\n",
    "\n",
    "    def forward(self, token_embeddings, attention_mask):\n",
    "        # token_embeddings: (batch_size, seq_length, input_dim)\n",
    "        # attention_mask: (batch_size, seq_length)\n",
    "\n",
    "        # Permute token_embeddings to match Transformer input requirements\n",
    "        token_embeddings = token_embeddings.permute(1, 0, 2)  # (seq_length, batch_size, input_dim)\n",
    "\n",
    "        # Convert attention mask to key_padding_mask (True for padding tokens)\n",
    "        key_padding_mask = attention_mask == 0  # Shape: (batch_size, seq_length)\n",
    "\n",
    "        # Pass through Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(\n",
    "            token_embeddings,\n",
    "            src_key_padding_mask=key_padding_mask  # (batch_size, seq_length)\n",
    "        )  # (seq_length, batch_size, input_dim)\n",
    "\n",
    "        # Transpose back to (batch_size, seq_length, input_dim)\n",
    "        transformer_output = transformer_output.permute(1, 0, 2)\n",
    "\n",
    "        # Predict motion parameters\n",
    "        root_pose = self.fc_root(transformer_output)           # (batch_size, seq_length, 3)\n",
    "        body_pose = self.fc_body(transformer_output)           # (batch_size, seq_length, 63)\n",
    "        left_hand_pose = self.fc_left_hand(transformer_output) # (batch_size, seq_length, 45)\n",
    "        right_hand_pose = self.fc_right_hand(transformer_output) # (batch_size, seq_length, 45)\n",
    "\n",
    "        return root_pose, body_pose, left_hand_pose, right_hand_pose\n",
    "\n",
    "# Function to load the trained model and embeddings\n",
    "def load_model_and_embeddings(model_path, embedding_path):\n",
    "    # Load the trained Transformer model\n",
    "    model = TextToMotionTransformer(input_dim=768)  # Ensure input_dim matches the one used during training\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load embeddings and attention masks\n",
    "    saved_data = torch.load(embedding_path, map_location=device)\n",
    "    train_embeddings = saved_data['train_embeddings']\n",
    "    train_attention_masks = saved_data['train_attention_masks']\n",
    "    test_embeddings = saved_data['test_embeddings']\n",
    "    test_attention_masks = saved_data['test_attention_masks']\n",
    "    validation_embeddings = saved_data['validation_embeddings']\n",
    "    validation_attention_masks = saved_data['validation_attention_masks']\n",
    "    \n",
    "    return model, train_embeddings, train_attention_masks, test_embeddings, test_attention_masks, validation_embeddings, validation_attention_masks\n",
    "\n",
    "# Specify the paths for the model and embeddings\n",
    "#model_path = r\"C:\\\\Users\\Admin\\\\Desktop\\\\text to motion transformer\\\\text_to_motion_transformer.pth\"\n",
    "#embedding_path = r\"C:\\\\Users\\\\Admin\\\\Desktop\\\\text to motion transformer\\\\embeddings_and_masks.pth\"  # Update with actual path to your embeddings\n",
    "\n",
    "# Load the model and embeddings\n",
    "#model, train_embeddings, train_attention_masks, test_embeddings, test_attention_masks, validation_embeddings, validation_attention_masks = load_model_and_embeddings(model_path, embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2628\\3684768033.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2628\\3684768033.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_data = torch.load(embedding_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# File paths for the saved model and embeddings\n",
    "model_path = \"text_to_motion_transformer.pth\"\n",
    "embedding_path = \"embeddings_and_masks.pth\"\n",
    "\n",
    "# Load the model and embeddings\n",
    "model, train_embeddings, train_attention_masks, test_embeddings, test_attention_masks, validation_embeddings, validation_attention_masks = load_model_and_embeddings(model_path, embedding_path)\n",
    "print(\"Model and embeddings loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer and model (same as used during training)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "# Function to generate BERT embeddings for a given text input\n",
    "def generate_bert_embeddings(text):\n",
    "    # Tokenize and generate token embeddings\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,  # Ensure this matches the MAX_SEQ_LENGTH used during training\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Get the token embeddings (last hidden state)\n",
    "    token_embeddings = outputs.last_hidden_state.squeeze(0)  # Remove batch dimension\n",
    "    attention_mask = inputs['attention_mask'].squeeze(0)     # Remove batch dimension\n",
    "\n",
    "    return token_embeddings, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Hand Pose Prediction:\n",
      " [[[-2.1505181e-03  1.1478206e-03 -4.1114371e-03 ... -8.7986584e-05\n",
      "    6.2221894e-05  9.1558543e-04]\n",
      "  [-2.1505186e-03  1.1478201e-03 -4.1114353e-03 ... -8.7985885e-05\n",
      "    6.2221778e-05  9.1558555e-04]\n",
      "  [-2.1505181e-03  1.1478201e-03 -4.1114353e-03 ... -8.7986351e-05\n",
      "    6.2221778e-05  9.1558555e-04]\n",
      "  ...\n",
      "  [-2.1505181e-03  1.1478201e-03 -4.1114353e-03 ... -8.7986235e-05\n",
      "    6.2221545e-05  9.1558549e-04]\n",
      "  [-2.1505184e-03  1.1478201e-03 -4.1114353e-03 ... -8.7986002e-05\n",
      "    6.2221894e-05  9.1558538e-04]\n",
      "  [-2.1505184e-03  1.1478201e-03 -4.1114353e-03 ... -8.7986118e-05\n",
      "    6.2221894e-05  9.1558549e-04]]]\n",
      "Right Hand Pose Prediction:\n",
      " [[[-0.00162534  0.00117521  0.00440383 ...  0.0003007   0.00041492\n",
      "   -0.00052171]\n",
      "  [-0.00162534  0.00117521  0.00440383 ...  0.0003007   0.00041492\n",
      "   -0.00052171]\n",
      "  [-0.00162534  0.00117521  0.00440383 ...  0.0003007   0.00041492\n",
      "   -0.00052171]\n",
      "  ...\n",
      "  [-0.00162534  0.00117521  0.00440383 ...  0.0003007   0.00041492\n",
      "   -0.00052171]\n",
      "  [-0.00162534  0.00117521  0.00440383 ...  0.0003007   0.00041492\n",
      "   -0.00052171]\n",
      "  [-0.00162534  0.00117521  0.00440383 ...  0.0003007   0.00041492\n",
      "   -0.00052171]]]\n",
      "Root Pose Prediction:\n",
      " [[[0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734381 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734384 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]\n",
      "  [0.03734383 0.00041058 0.00052578]]]\n",
      "Body Pose Prediction:\n",
      " [[[-0.00273272  0.00137826  0.00258999 ... -0.00183896  0.0010862\n",
      "   -0.00327089]\n",
      "  [-0.00273272  0.00137826  0.00258999 ... -0.00183895  0.0010862\n",
      "   -0.00327089]\n",
      "  [-0.00273272  0.00137826  0.00258999 ... -0.00183895  0.0010862\n",
      "   -0.00327089]\n",
      "  ...\n",
      "  [-0.00273272  0.00137826  0.00258999 ... -0.00183895  0.0010862\n",
      "   -0.00327089]\n",
      "  [-0.00273272  0.00137826  0.00258999 ... -0.00183895  0.0010862\n",
      "   -0.00327089]\n",
      "  [-0.00273272  0.00137826  0.00258999 ... -0.00183895  0.0010862\n",
      "   -0.00327089]]]\n"
     ]
    }
   ],
   "source": [
    "# Example text input\n",
    "text_input = \"And just let those fingers relax\"\n",
    "\n",
    "# Generate BERT embeddings for the input text\n",
    "text_embeddings, attention_mask = generate_bert_embeddings(text_input)\n",
    "\n",
    "# Reshape embeddings and attention mask to match the expected input format\n",
    "text_embeddings = text_embeddings.unsqueeze(0)  # Shape: (1, seq_length, hidden_size)\n",
    "attention_mask = attention_mask.unsqueeze(0)    # Shape: (1, seq_length)\n",
    "\n",
    "# Move to the device\n",
    "text_embeddings = text_embeddings.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Pass through the model to generate motion parameters\n",
    "with torch.no_grad():\n",
    "    root_pose_pred, body_pose_pred, left_hand_pred, right_hand_pred = model(text_embeddings, attention_mask)\n",
    "\n",
    "# Print the predicted motion parameters\n",
    "print(\"Left Hand Pose Prediction:\\n\", left_hand_pred.cpu().numpy())\n",
    "print(\"Right Hand Pose Prediction:\\n\", right_hand_pred.cpu().numpy())\n",
    "print(\"Root Pose Prediction:\\n\", root_pose_pred.cpu().numpy())\n",
    "print(\"Body Pose Prediction:\\n\", body_pose_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "\n",
    "# Function to visualize the motion sequence in 3D with debugging\n",
    "def visualize_motion_sequence_debug(root_pose, body_pose, left_hand_pose, right_hand_pose):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Print out the values for debugging\n",
    "    print(\"Root Pose Sample Frame Values:\\n\", root_pose[0].cpu().numpy())\n",
    "    print(\"Body Pose Sample Frame Values:\\n\", body_pose[0].cpu().numpy())\n",
    "    print(\"Left Hand Pose Sample Frame Values:\\n\", left_hand_pose[0].cpu().numpy())\n",
    "    print(\"Right Hand Pose Sample Frame Values:\\n\", right_hand_pose[0].cpu().numpy())\n",
    "    \n",
    "    # Iterate over frames to plot motion sequence\n",
    "    for frame_idx in range(root_pose.shape[0]):\n",
    "        ax.clear()\n",
    "\n",
    "        # Convert tensors to CPU and then to NumPy for visualization\n",
    "        root_pos_cpu = root_pose[frame_idx].cpu().numpy()\n",
    "        body_pos_cpu = body_pose[frame_idx].cpu().numpy()\n",
    "        left_hand_pos_cpu = left_hand_pose[frame_idx].cpu().numpy()\n",
    "        right_hand_pos_cpu = right_hand_pose[frame_idx].cpu().numpy()\n",
    "\n",
    "        # Plot root position (increase point size 's')\n",
    "        ax.scatter(root_pos_cpu[0], root_pos_cpu[1], root_pos_cpu[2], c='r', s=100, label='Root Pose')\n",
    "\n",
    "        # Plot body pose (every 3rd value to get x, y, z positions)\n",
    "        ax.scatter(body_pos_cpu[::3], body_pos_cpu[1::3], body_pos_cpu[2::3], c='b', s=50, label='Body Pose')\n",
    "\n",
    "        # Plot left hand pose (every 3rd value)\n",
    "        ax.scatter(left_hand_pos_cpu[::3], left_hand_pos_cpu[1::3], left_hand_pos_cpu[2::3], c='g', s=50, label='Left Hand Pose')\n",
    "\n",
    "        # Plot right hand pose (every 3rd value)\n",
    "        ax.scatter(right_hand_pos_cpu[::3], right_hand_pos_cpu[1::3], right_hand_pos_cpu[2::3], c='m', s=50, label='Right Hand Pose')\n",
    "\n",
    "        # Set expanded axis limits to ensure visibility\n",
    "        ax.set_xlim([-1, 1])  # Adjust based on your data range\n",
    "        ax.set_ylim([-1, 1])  # Adjust based on your data range\n",
    "        ax.set_zlim([-1, 1])  # Adjust based on your data range\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        ax.set_title(f\"Frame {frame_idx + 1}\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Pause to create an animation effect\n",
    "        plt.pause(0.5)  # Increase pause time for better observation\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "# Call the visualization function\n",
    "# visualize_motion_sequence_debug(\n",
    "#     root_pose_pred[0],        # Shape: (seq_length, 3)\n",
    "#     body_pose_pred[0],        # Shape: (seq_length, 63)\n",
    "#     left_hand_pred[0],        # Shape: (seq_length, 45)\n",
    "#     right_hand_pred[0]        # Shape: (seq_length, 45)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smplx\n",
    "import torch\n",
    "\n",
    "# Set paths and device\n",
    "model_path = \"C:\\\\Users\\\\Admin\\\\Desktop\\\\text to motion transformer\\\\models\\\\smplx\\\\SMPLX_MALE.pkl\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load SMPL-X model\n",
    "#model = smplx.SMPLX(model_path=model_path, gender='neutral', use_pca=False)\n",
    "model = smplx.SMPLX(model_path=model_path, gender='neutral', use_pca=False, ext='pkl')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embeddings Shape: torch.Size([1, 1, 8, 768])\n",
      "Attention Mask Shape: torch.Size([1, 1, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1, 3]' is invalid for input of size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Pass through the model to generate motion parameters\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 29\u001b[0m     root_pose_pred, body_pose_pred, left_hand_pred, right_hand_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Check shapes for predicted pose parameters\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoot Pose Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_pose_pred\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)        \u001b[38;5;66;03m# Shape: (seq_len, 3)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\smplx\\body_models.py:1208\u001b[0m, in \u001b[0;36mSMPLX.forward\u001b[1;34m(self, betas, global_orient, body_pose, left_hand_pose, right_hand_pose, transl, expression, jaw_pose, leye_pose, reye_pose, return_verts, return_full_pose, pose2rot, return_shaped, **kwargs)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     left_hand_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[0;32m   1204\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi,ij->bj\u001b[39m\u001b[38;5;124m'\u001b[39m, [left_hand_pose, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_hand_components])\n\u001b[0;32m   1205\u001b[0m     right_hand_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[0;32m   1206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi,ij->bj\u001b[39m\u001b[38;5;124m'\u001b[39m, [right_hand_pose, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_hand_components])\n\u001b[1;32m-> 1208\u001b[0m full_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mglobal_orient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1209\u001b[0m                        body_pose\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNUM_BODY_JOINTS, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m   1210\u001b[0m                        jaw_pose\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m   1211\u001b[0m                        leye_pose\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m   1212\u001b[0m                        reye_pose\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m   1213\u001b[0m                        left_hand_pose\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m   1214\u001b[0m                        right_hand_pose\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m3\u001b[39m)],\n\u001b[0;32m   1215\u001b[0m                       dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m165\u001b[39m)\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# Add the mean pose of the model. Does not affect the body, only the\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;66;03m# hands when flat_hand_mean == False\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m full_pose \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpose_mean\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 1, 3]' is invalid for input of size 8"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import smplx\n",
    "\n",
    "# Example: Text input\n",
    "text_input = \"And just let those fingers relax\"\n",
    "\n",
    "# Generate BERT embeddings for the input text\n",
    "text_embeddings, attention_mask = generate_bert_embeddings(text_input)\n",
    "\n",
    "# Reshape embeddings to match the expected format\n",
    "text_embeddings = text_embeddings.unsqueeze(0)  # Shape: (1, seq_len, hidden_size)\n",
    "attention_mask = attention_mask.unsqueeze(0)    # Shape: (1, seq_len)\n",
    "\n",
    "# Check shapes\n",
    "print(f\"Text Embeddings Shape: {text_embeddings.shape}\")\n",
    "print(f\"Attention Mask Shape: {attention_mask.shape}\")\n",
    "\n",
    "# Load SMPL-X model (adjust the path as necessary)\n",
    "model_path = \"C:\\\\Users\\\\Admin\\\\Desktop\\\\text to motion transformer\\\\models\\\\smplx\\\\SMPLX_MALE.pkl\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = smplx.SMPLX(model_path=model_path, gender='neutral', use_pca=False, ext='pkl')\n",
    "model = model.to(device)\n",
    "\n",
    "# Pass through the model to generate motion parameters\n",
    "with torch.no_grad():\n",
    "    root_pose_pred, body_pose_pred, left_hand_pred, right_hand_pred = model(text_embeddings, attention_mask)\n",
    "\n",
    "# Check shapes for predicted pose parameters\n",
    "print(f\"Root Pose Shape: {root_pose_pred.shape}\")        # Shape: (seq_len, 3)\n",
    "print(f\"Body Pose Shape: {body_pose_pred.shape}\")        # Shape: (seq_len, 63)\n",
    "print(f\"Left Hand Pose Shape: {left_hand_pred.shape}\")   # Shape: (seq_len, 45)\n",
    "print(f\"Right Hand Pose Shape: {right_hand_pred.shape}\") # Shape: (seq_len, 45)\n",
    "\n",
    "# Video generation function\n",
    "def generate_motion_video(root_pose, body_pose, left_hand_pose, right_hand_pose):\n",
    "    canvas_width, canvas_height = 600, 600\n",
    "    video_filename = \"motion_video.avi\"\n",
    "    \n",
    "    # Set up video writer (OpenCV)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    video_writer = cv2.VideoWriter(video_filename, fourcc, 20.0, (640, 480))\n",
    "\n",
    "    for frame_idx in range(root_pose.shape[0]):\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Plotting root pose, body pose, and hand poses\n",
    "        ax.scatter(root_pose[frame_idx, 0], root_pose[frame_idx, 1], root_pose[frame_idx, 2], c='r', s=100, label='Root Pose')\n",
    "        ax.scatter(body_pose[frame_idx, ::3], body_pose[frame_idx, 1::3], body_pose[frame_idx, 2::3], c='b', s=50, label='Body Pose')\n",
    "        ax.scatter(left_hand_pose[frame_idx, ::3], left_hand_pose[frame_idx, 1::3], left_hand_pose[frame_idx, 2::3], c='g', s=50, label='Left Hand Pose')\n",
    "        ax.scatter(right_hand_pose[frame_idx, ::3], right_hand_pose[frame_idx, 1::3], right_hand_pose[frame_idx, 2::3], c='m', s=50, label='Right Hand Pose')\n",
    "\n",
    "        ax.set_xlim([-1, 1])\n",
    "        ax.set_ylim([-1, 1])\n",
    "        ax.set_zlim([-1, 1])\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        ax.set_title(f\"Frame {frame_idx + 1}\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Capture the current frame as an image\n",
    "        plt_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        plt_img = plt_img.reshape(canvas_height, canvas_width, 3)\n",
    "\n",
    "        print(f\"Captured image size: {plt_img.size}, Expected size: {canvas_height * canvas_width * 3}\")\n",
    "\n",
    "        # Resize image to match the video frame size\n",
    "        resized_img = cv2.resize(plt_img, (640, 480))\n",
    "\n",
    "        # Write the resized image to the video\n",
    "        video_writer.write(resized_img)\n",
    "        \n",
    "        # Close the plot\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Release the video writer\n",
    "    video_writer.release()\n",
    "    print(\"Video generation complete.\")\n",
    "\n",
    "# Example: Call the function with predicted pose data\n",
    "generate_motion_video(root_pose_pred.cpu().numpy(), body_pose_pred.cpu().numpy(), left_hand_pred.cpu().numpy(), right_hand_pred.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
